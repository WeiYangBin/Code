```
"""Extended Memory Network."""
import numpy as np
import tensorflow as tf


class EMN(object):
    """Implementation of E-MN model."""

    def __init__(self, config):
        """Init model."""
        self.word_dim = config['word_dim']
        self.vocab_num = config['vocab_num']
        self.pretrained_embedding = config['pretrained_embedding']
        self.video_feature_dim = config['video_feature_dim']
        self.video_feature_num = config['video_feature_num']
        self.memory_dim = config['memory_dim']
        self.answer_num = config['answer_num']

        self.video_feature = None
        self.question_encode = None
        self.answer_encode = None

        self.logit = None
        self.prediction = None
        self.loss = None
        self.acc = None

        self.train = None

        self.appear_dim = config['video_feature_dim']  # 4096
        self.motion_dim = config['video_feature_dim']  # 4096
        self.frame_num = config['video_feature_num']   # 20
        self.clip_num = config['video_feature_num']    # 20
        self.common_dim = config['memory_dim']         # 256


    def build_inference(self, is_train):
        """Build inference graph."""
        with tf.name_scope('input'):
            self.appear = tf.placeholder(
                tf.float32, [None, self.frame_num, self.appear_dim], 'appear')
            self.motion = tf.placeholder(
                tf.float32, [None, self.clip_num, self.motion_dim], 'motion')
            self.question_encode = tf.placeholder(
                tf.int32, [None, None], 'question_encode')

        with tf.variable_scope('process_question'):
            if self.pretrained_embedding:
                embedding_matrix = tf.get_variable(
                    'embedding_matrix',
                    initializer=np.load(self.pretrained_embedding),
                    regularizer=tf.nn.l2_loss)
            else:
                embedding_matrix = tf.get_variable(
                    'embedding_matrix', [self.vocab_num, self.word_dim],
                    regularizer=tf.nn.l2_loss)
            question_embedding = tf.nn.embedding_lookup(
                embedding_matrix, self.question_encode, name='question_embedding')
            cell = tf.nn.rnn_cell.BasicLSTMCell(self.word_dim)
            # cell = tf.nn.rnn_cell.GRUCell(self.word_dim)
            _, question_state = tf.nn.dynamic_rnn(
                cell, question_embedding, dtype=tf.float32, scope='lstm')

        with tf.variable_scope('process_video'):
            with tf.variable_scope('appear'):
                cell_fw = tf.nn.rnn_cell.BasicLSTMCell(self.memory_dim)  # self.video_feature_dim
                cell_bw = tf.nn.rnn_cell.BasicLSTMCell(self.memory_dim)  # self.video_feature_dim
                batch_size = tf.shape(self.appear)[0] 
                seq_len = tf.fill(
                    [1, batch_size], tf.constant(self.frame_num, dtype=tf.int64))[0]
                bi_appear, _ = tf.nn.bidirectional_dynamic_rnn(
                    cell_fw, cell_bw, self.appear, seq_len, dtype=tf.float32, scope='bi_appear_lstm')
                bi_appear = (bi_appear[0] + bi_appear[1]) / 2
                # bi_appear = tf.reduce_sum(tf.stack(bi_appear), axis = 0)

            with tf.variable_scope('motion'):
                cell_fw = tf.nn.rnn_cell.BasicLSTMCell(self.memory_dim)  # self.video_feature_dim
                cell_bw = tf.nn.rnn_cell.BasicLSTMCell(self.memory_dim)  # self.video_feature_dim
                batch_size = tf.shape(self.motion)[0] 
                seq_len = tf.fill(
                    [1, batch_size], tf.constant(self.clip_num, dtype=tf.int64))[0]
                bi_motion, _ = tf.nn.bidirectional_dynamic_rnn(
                    cell_fw, cell_bw, self.motion, seq_len, dtype=tf.float32, scope='bi_motion_lstm')
                bi_motion = (bi_motion[0] + bi_motion[1]) / 2  
                # bi_motion = tf.reduce_mean(tf.stack(bi_motion), axis = 0)
                    

        with tf.variable_scope('memory'):
            A1 = tf.get_variable(
                'A1', [self.memory_dim, self.memory_dim], 
                regularizer=tf.nn.l2_loss)  # [self.video_feature_dim, self.memory_dim]
            C1A2 = tf.get_variable(
                'C1A2', [self.memory_dim, self.memory_dim],
                regularizer=tf.nn.l2_loss)  # [self.video_feature_dim, self.memory_dim]
            C2A3 = tf.get_variable(
                'C2A3', [self.memory_dim, self.memory_dim],
                regularizer=tf.nn.l2_loss)  # [self.video_feature_dim, self.memory_dim]
            C3A4 = tf.get_variable(
                'C3A4', [self.memory_dim, self.memory_dim],
                regularizer=tf.nn.l2_loss)  # [self.video_feature_dim, self.memory_dim]
            C4 = tf.get_variable(
                'C4', [self.memory_dim, self.memory_dim],
                regularizer=tf.nn.l2_loss)  # [self.video_feature_dim, self.memory_dim]
            B = tf.get_variable(
                'B', [self.word_dim, self.memory_dim],
                regularizer=tf.nn.l2_loss) 

            u1 = tf.matmul(question_state.c, B, name='u1')

            with tf.variable_scope('episodic_appear') as scope:
                # get longest sentence size
                max_sentence_size = tf.shape(bi_appear)[1]

                # convert question from shape [BATCH_SIZE, CELL_SIZE] to [BATCH_SIZE, MAX_SENTENCE_SIZE, CELL_SIZE]
                question_tiled = tf.tile(tf.reshape(u1, [-1, 1, self.memory_dim]), [1, max_sentence_size, 1])

                # initialize weights for attention
                w1 = tf.Variable(tf.truncated_normal([1, 4 * self.memory_dim, self.memory_dim], stddev=0.05))
                b1 = tf.Variable(tf.constant(0.01, shape=[1, 1, self.memory_dim]))

                w2 = tf.Variable(tf.truncated_normal([1, self.memory_dim, 1], stddev=0.05))
                b2 = tf.Variable(tf.constant(0.01, shape=[1, 1, 1]))

                def attention(memory_appear, reuse):
                    with tf.variable_scope("attention", reuse=reuse):
                        # extend and tile memory to [BATCH_SIZE, MAX_SENTENCE_SIZE, CELL_SIZE] shape
                        memory_appear = tf.tile(tf.reshape(memory_appear, [-1, 1, self.memory_dim]), [1, max_sentence_size, 1])

                        # interactions between facts, memory and question as described in paper
                        attending = tf.concat([bi_appear * memory_appear, bi_appear * question_tiled,
                           tf.abs(bi_appear - memory_appear), tf.abs(bi_appear - question_tiled)], 2)

                        # get current batch size
                        batch = tf.shape(attending)[0]

                        # first fully connected layer
                        h1 = tf.matmul(attending, tf.tile(w1, [batch, 1, 1]))
                        h1 = h1 + tf.tile(b1, [batch, max_sentence_size, 1])
                        h1 = tf.nn.tanh(h1)

                        # second and final fully connected layer
                        h2 = tf.matmul(h1, tf.tile(w2, [batch, 1, 1]))
                        h2 = h2 + tf.tile(b2, [batch, max_sentence_size, 1])

                        # returns softmax so attention scores are from 0 to 1
                        return tf.nn.softmax(h2, 1)

                # first memory state is question state
                memory_appear = u1
                # memory_appear = tf.reshape(bi_appear, [-1, self.memory_dim])
                att = []

                for p in range(3):
                    episode = tf.reduce_sum(tf.expand_dims(attention(memory_appear, bool(p)), 2) * memory_appear, 2)
                    episode = tf.reshape(episode, [-1, 20 * self.memory_dim])
                    w_epizode = tf.Variable(tf.truncated_normal([20 * self.memory_dim, self.memory_dim], stddev=0.05))
                    b_epizode = tf.Variable(tf.constant(0.01, shape=[self.memory_dim]))
                    m = tf.matmul(episode, w_epizode) + b_epizode
                    # new memory state is dependent on previous, current memory and question
                    #ppear = tf.nn.relu(m) 
                    if is_train == 1:
                        memory_appear = tf.nn.relu(m)
                        memory_appear = tf.nn.dropout(memory_appear, keep_prob=0.5)        
                    if is_train == 0:
                        memory_appear = tf.nn.relu(m)

            with tf.variable_scope('episodic_motion') as scope:
                # get longest sentence size
                max_sentence_size = tf.shape(bi_motion)[1]

                # convert question from shape [BATCH_SIZE, CELL_SIZE] to [BATCH_SIZE, MAX_SENTENCE_SIZE, CELL_SIZE]
                question_tiled = tf.tile(tf.reshape(u1, [-1, 1, self.memory_dim]), [1, max_sentence_size, 1])

                # initialize weights for attention
                w1 = tf.Variable(tf.truncated_normal([1, 4 * self.memory_dim, self.memory_dim], stddev=0.05))
                b1 = tf.Variable(tf.constant(0.01, shape=[1, 1, self.memory_dim]))

                w2 = tf.Variable(tf.truncated_normal([1, self.memory_dim, 1], stddev=0.05, seed=1))
                b2 = tf.Variable(tf.constant(0.01, shape=[1, 1, 1]))

                def attention(memory_motion, reuse):
                    with tf.variable_scope("attention", reuse=reuse):
                        # extend and tile memory to [BATCH_SIZE, MAX_SENTENCE_SIZE, CELL_SIZE] shape
                        memory_motion = tf.tile(tf.reshape(memory_motion, [-1, 1, self.memory_dim]), [1, max_sentence_size, 1])

                        # interactions between facts, memory and question as described in paper
                        attending = tf.concat([bi_motion * memory_motion, bi_motion * question_tiled,
                           tf.abs(bi_motion - memory_motion), tf.abs(bi_motion - question_tiled)], 2)

                        # get current batch size
                        batch = tf.shape(attending)[0]

                        # first fully connected layer
                        h1 = tf.matmul(attending, tf.tile(w1, [batch, 1, 1]))
                        h1 = h1 + tf.tile(b1, [batch, max_sentence_size, 1])
                        h1 = tf.nn.tanh(h1)

                        # second and final fully connected layer
                        h2 = tf.matmul(h1, tf.tile(w2, [batch, 1, 1]))
                        h2 = h2 + tf.tile(b2, [batch, max_sentence_size, 1])

                        # returns softmax so attention scores are from 0 to 1
                        return tf.nn.softmax(h2, 1)

                # first memory state is question state
                memory_motion = u1
                # memory_motion = tf.reshape(bi_motion, [-1, self.memory_dim])
                att = []

                for p in range(3):
                    episode = tf.reduce_sum(tf.expand_dims(attention(memory_motion, bool(p)), 2) * memory_motion, 2)
                    episode = tf.reshape(episode, [-1, 20 * self.memory_dim])
                    w_epizode = tf.Variable(tf.truncated_normal([20 * self.memory_dim, self.memory_dim], stddev=0.05))
                    b_epizode = tf.Variable(tf.constant(0.01, shape=[self.memory_dim]))
                    m = tf.matmul(episode, w_epizode) + b_epizode
                    #memory_motion = tf.nn.relu(m) 
                    if is_train == 1:
                        memory_motion = tf.nn.relu(m)
                        memory_motion = tf.nn.dropout(memory_motion, keep_prob=0.5)
                    if is_train == 0:
                        memory_motion = tf.nn.relu(m) 
            
            fuse = tf.add(memory_appear, memory_motion, name='fuse')
            fuse = tf.multiply(fuse, u1)

            # memory_appear = tf.multiply(memory_appear, u1)
            # memory_motion = tf.multiply(memory_motion, u1)
            # fuse = tf.add(memory_appear, memory_motion, name='fuse')

        with tf.variable_scope('output'):
            W = tf.get_variable(
                'W', [self.memory_dim, self.answer_num],
                regularizer=tf.nn.l2_loss)
            b = tf.get_variable('b', [self.answer_num])
            self.logit = tf.nn.softmax(
                tf.nn.xw_plus_b(fuse, W, b), name='logit')
            self.prediction = tf.argmax(self.logit, axis=1, name='prediction')

    def build_loss(self, reg_coeff):
        """Compute loss and acc."""
        with tf.name_scope('answer'):
            self.answer_encode = tf.placeholder(
                tf.int64, [None], 'answer_encode')
            answer_one_hot = tf.one_hot(
                self.answer_encode, self.answer_num)
        with tf.name_scope('loss'):
            log_loss = tf.losses.log_loss(
                answer_one_hot, self.logit, scope='log_loss')
            reg_loss = tf.add_n(
                tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name='reg_loss')
            self.loss = log_loss + reg_coeff * reg_loss
        with tf.name_scope("acc"):
            correct = tf.equal(self.prediction, self.answer_encode)
            self.acc = tf.reduce_mean(tf.cast(correct, "float"))

    def build_train(self, learning_rate):
        """
        Add train operation.
        warm_up
        """
        with tf.variable_scope('train'):
            global_steps = tf.Variable(0, trainable=False)                        
            warm_up_steps = 1900    #msvd = 190    msrvtt = 510
            warm_up_lr = learning_rate #0.001
            first_normal_lr = 0.005   
            ratio = tf.to_float(global_steps) / tf.to_float(warm_up_steps)
            warm_up_learning_rate = warm_up_lr + ratio * (first_normal_lr - warm_up_lr)
            decay_learning_rate = tf.train.exponential_decay(learning_rate=0.005, 
                                                             global_step=global_steps, 
                                                             decay_steps=935,  #msvd = 935    msrvtt = 2543
                                                             decay_rate=0.8,    #0.8
                                                             staircase=False)
            is_normal = global_steps >= warm_up_steps
            learning_rate = tf.cond(is_normal, lambda: decay_learning_rate, lambda: warm_up_learning_rate)
            vs_optimizer = tf.train.AdamOptimizer(learning_rate, name='vs_adam')
            # vs_train_op = vs_optimizer.minimize(self.loss, global_step=global_steps)
            self.train = vs_optimizer.minimize(self.loss, global_step=global_steps)
            # grads_and_vars = vs_optimizer.compute_gradients(self.loss)

    def attend(self, target, sources, name=None):
        """Use target to attend on sources. `target` and `sources` should have equal dim.

        Args:
            target: [None, target_dim].
            sources: [None, source_num, source_dim].
        Returns:
            weight: [None, source_num].
            att: [None, source_dim].
        """
        with tf.name_scope(name, 'attend'):
            weight = tf.nn.softmax(tf.reduce_sum(
                tf.expand_dims(target, 1) * sources, 2))
            att = tf.reduce_sum(
                tf.expand_dims(weight, 2) * sources, 1)
            return weight, att

```
